import asyncio
import os
import json
import argparse
from openai import AsyncOpenAI
from dotenv import load_dotenv
from mcp.client.session import ClientSession
from mcp.client.streamable_http import streamablehttp_client
import traceback #å¼‚å¸¸å¤„ç†


load_dotenv()
mcp_server_url = os.getenv("MCP_SERVER_URL")

# é€šè¿‡ç”¨æˆ·-q --questions è¯»å–é—®é¢˜
question = []

DEFAULT_QUESTION = ["ç°åœ¨æ·±åœ³çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"]

class MCPAgent():
    def __init__(self, openai_client, mcp_session=None):
        self.openai_client = openai_client
        self.mcp_session = mcp_session
        self.available_tools = []



    @property
    def mcp_available(self):
        return self.mcp_session is not None



    def get_openai_tools_schema(self):
        """
        å°†MCPå·¥å…·è½¬åŒ–ä¸ºOpenAIå‡½æ•°è°ƒç”¨æ ¼å¼
        """
        if not self.mcp_available or not self.available_tools:
            print("MCPä¸å¯ç”¨æˆ–æ— å¯ç”¨å·¥å…·ï¼Œè¿”å›ç©ºå·¥å…·åˆ—è¡¨")
            return []
            
        try:
            # åŠ è½½schemaå‚æ•°å®šä¹‰
            try:
                with open("schemas.json", "r", encoding="utf-8") as f:
                    schema_params = json.load(f)
            except FileNotFoundError:
                print("schemas.jsonæ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤schema")
                schema_params = {}
            
            openai_tools = []
            print(f"å¼€å§‹æ„å»ºOpenAIå·¥å…·schemaï¼Œå¯ç”¨å·¥å…·æ•°é‡: {len(self.available_tools)}")
            
            for tool in self.available_tools:
                print(f"å¤„ç†å·¥å…·: '{tool.name}'")
                
                # è·å–è¯¥å·¥å…·çš„schemaå‚æ•°
                tool_params = schema_params.get(tool.name, {})
                
                # æ„å»ºOpenAIå·¥å…·çš„schema
                tool_schema = {
                    "type": "function",
                    "function": {
                        "name": tool.name,
                        "description": tool.description or f"MCP tool: {tool.name}",
                        "parameters": tool_params if tool_params else self._get_default_schema(tool)
                    }
                }        
                openai_tools.append(tool_schema)
            
            print(f"æ€»å…±æ„å»ºäº† {len(openai_tools)} ä¸ªå·¥å…·schema")
            return openai_tools
            
        except Exception as e:
            print(f"æ„å»ºå·¥å…·schemaé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _get_default_schema(self, tool):
        """ä¸ºå·¥å…·ç”Ÿæˆé»˜è®¤çš„schema"""
        return {
            "type": "object",
            "properties": {},
            "required": []
        }

    async def call_map_tool(self, tool_name, parameters):
        if not self.mcp_available or not self.mcp_session:
            raise Exception("MCPæœåŠ¡å™¨ä¸å¯ç”¨ï¼Œæ— æ³•è°ƒç”¨å·¥å…·")
            
        try:    
            # è·å–å·¥å…·è¯¦ç»†ä¿¡æ¯
            result = await self.mcp_session.call_tool(tool_name, parameters)

            if result.content and len(result.content) > 0:
                content_text = result.content[0].text
                # print(f"å·¥å…·è¿”å›å†…å®¹: {content_text}")
                return content_text
            else:
                return "å·¥å…·è°ƒç”¨æˆåŠŸï¼Œä½†æ— è¿”å›ç»“æœ"
                
        except Exception as e:
            print(f"MCPå·¥å…·è°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            raise e
    
    async def chat_with_tools(self, user_message):
        #åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯
        system_content = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ã€‚"
        if self.mcp_available and self.available_tools:
            system_content += "ä½ æœ‰ä¸€äº›å·¥å…·å¯ä»¥å¸®åŠ©è·å–å®æ—¶ä¿¡æ¯ï¼Œå¦‚å¤©æ°”æŸ¥è¯¢ç­‰ã€‚"
        else:
            system_content += "è™½ç„¶ä½ æ²¡æœ‰å®æ—¶æ•°æ®è®¿é—®èƒ½åŠ›ï¼Œä½†ä½ ä¼šåŸºäºä½ çš„çŸ¥è¯†å°½åŠ›å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            
        messages = [
            {
                "role": "system", 
                "content": system_content
            },
            {
                "role":"user",
                "content":user_message
            }
        ]

        tools = self.get_openai_tools_schema()
        print(f"ğŸ¯ å‡†å¤‡è°ƒç”¨OpenAIï¼Œä¼ é€’å·¥å…·æ•°é‡: {len(tools)}")
        if tools:
            print("ä¼ é€’çš„å·¥å…·åˆ—è¡¨:")
            for i, tool in enumerate(tools, 1):
                print(f"  {i}. {tool['function']['name']}: {tool['function']['description']}")
        else:
            print("æ²¡æœ‰å·¥å…·ä¼ é€’ç»™OpenAIï¼ˆMCPä¸å¯ç”¨æˆ–æ— å¯ç”¨å·¥å…·ï¼‰")

        try:
            #é¦–æ¬¡è°ƒç”¨openAI
            print("å¼€å§‹è°ƒç”¨OpenAI API...")
            
            # æ ¹æ®æ˜¯å¦æœ‰å·¥å…·æ¥å†³å®šè°ƒç”¨å‚æ•°
            if tools:
                respond = await self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages = messages,
                    tools = tools,
                    tool_choice="auto",
                    max_tokens=500
                )
            else:
                # æ²¡æœ‰å·¥å…·æ—¶ä¸ä¼ é€’toolså‚æ•°
                respond = await self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages = messages,
                    max_tokens=500
                )            
            respond_message = respond.choices[0].message 
            print(f"å·¥å…·è°ƒç”¨æ•°é‡: {len(respond_message.tool_calls) if respond_message.tool_calls else 0}")
            print(f"OpenAIå“åº”æ¶ˆæ¯: {respond_message}")
            
            #æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            if respond_message.tool_calls:
                messages.append({
                    "role":"assistant",
                    "content": respond_message.content,
                    "tool_calls":[{
                        "id": tool_call.id,
                        "type":tool_call.type, 
                        "function":{
                            "name":tool_call.function.name,
                            "arguments":tool_call.function.arguments
                        }
                    }for tool_call in respond_message.tool_calls]
                })


                for tool_call in respond_message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)
                    
                    print(f"è°ƒç”¨å·¥å…·: {function_name}")
                    print(f"å·¥å…·å‚æ•°: {function_args}")

                    try:
                        tool_result = await self.call_map_tool(function_name, function_args)
                        
                        #è®°å½•å·¥å…·è°ƒç”¨ç»“æœ
                        messages.append({
                            "role":"tool",
                            "tool_call_id":tool_call.id,
                            "content":str(tool_result)
                        })
                    except Exception as e:
                        error_msg = f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"
                        print(error_msg)
                        print(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {str(e)}")
                        
                        #è®°å½•å·¥å…·è°ƒç”¨å¤±è´¥ç»“æœ
                        messages.append({
                            "role":"tool",
                            "tool_call_id":tool_call.id,
                            "content":error_msg
                        })
                print(f"äºŒæ¬¡è°ƒç”¨OpenAIå‰çš„messages:")
                for message in messages:
                    print(message)
                #ç¬¬äºŒæ¬¡è°ƒç”¨ OpenAI,ç»“åˆç”¨æˆ·æé—®å’ŒMCPå·¥å…·è¿”å›çš„å†…å®¹
                final_response = await self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages = messages,
                    max_tokens = 300
                )
                return final_response.choices[0].message.content
            else:
                return respond_message.content 
        except Exception as e:
            return f"å¯¹è¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:{e}"

        





async def main():

    openai_client = AsyncOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL")
    )
    
    agent = None
    mcp_connection_success = False
    
    # å°è¯•è¿æ¥MCPæœåŠ¡å™¨
    if mcp_server_url:
        try:
            print(f"å°è¯•è¿æ¥åˆ°MCPæœåŠ¡å™¨: {mcp_server_url}")
            async with streamablehttp_client(mcp_server_url) as (read_stream, write_stream, _):
                async with ClientSession(read_stream, write_stream,) as mcp_session:
                    
                    #åˆå§‹åŒ–MCPè¿æ¥
                    print("æ­£åœ¨åˆå§‹åŒ–MCPè¿æ¥...")
                    await mcp_session.initialize()
                    print("MCPè¿æ¥åˆå§‹åŒ–æˆåŠŸ")
                    mcp_connection_success = True
                    
                    #è·å–MCPæœåŠ¡å™¨å·¥å…·
                    print("æ­£åœ¨è·å–MCPæœåŠ¡å™¨å·¥å…·...")
                    try:
                        tools_response = await mcp_session.list_tools()
                        available_tools = tools_response.tools
                    except Exception as e:
                        print(f"è·å–MCPå·¥å…·å¤±è´¥: {type(e).__name__}: {str(e)}")
                        available_tools = []

                    print(f"è¿æ¥MCPæœåŠ¡å™¨: {mcp_server_url}")
                    print(f"å¯ç”¨å·¥å…·æ•°é‡: {len(available_tools)}")
                    
                    if available_tools:
                        print("å¯ç”¨å·¥å…·åˆ—è¡¨:")
                        for i, tool in enumerate(available_tools, 1):
                            print(f"  {i}. åç§°: '{tool.name}'")
                            print(f"     æè¿°: '{tool.description}'")
                            print()
                    else:
                        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„MCPå·¥å…·")
                    
                    # åˆ›å»ºMCP Agent
                    agent = MCPAgent(openai_client, mcp_session)
                    # è®¾ç½®å¯ç”¨å·¥å…·
                    agent.available_tools = available_tools
                    
                    # å¤„ç†ç”¨æˆ·é—®é¢˜
                    await process_questions(agent)
                    
        except Exception as e:
            print(f"MCPè¿æ¥é”™è¯¯: {type(e).__name__}: {str(e)}")
            print("å°†ä½¿ç”¨æ— MCPæ¨¡å¼ç»§ç»­è¿è¡Œ...")
            mcp_connection_success = False
    else:
        print("æœªè®¾ç½®MCP_SERVER_URLï¼Œè·³è¿‡MCPè¿æ¥")
    
    # å¦‚æœMCPè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æ— MCPæ¨¡å¼
    if not mcp_connection_success:
        print("\n" + "="*60)
        print("ä½¿ç”¨æ— MCPæ¨¡å¼è¿è¡Œ")
        print("="*60)
        agent = MCPAgent(openai_client, None)
        await process_questions(agent)





async def process_questions(agent):
    for query in question:
        print("\n" + "="*60)
        print(f"ç”¨æˆ·æé—®:{query}")
        print("="*60)

        try:
            response = await agent.chat_with_tools(query)
            print("\n" + "-"*60)
            print(f"LLMå›ç­”: {response}")
            print("-"*60)        
            
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜å‡ºé”™{e}")
            traceback.print_exc()

def parse_arguments():
    parser = argparse.ArgumentParser(
        description="MCP Agent - æ™ºèƒ½åŠ©æ‰‹"
    )
    parser.add_argument(
        "-q", "--question",
        type=str,
        help="è¦è¯¢é—®çš„é—®é¢˜",
        metavar="é—®é¢˜å†…å®¹"
    )
    return parser.parse_args()








if __name__ == "__main__":
    
    #è§£æç”¨æˆ·æå‡ºçš„é—®é¢˜
    args = parse_arguments()

    if args.question:
        question = [args.question]
        print(f"--------------å¯åŠ¨ MCP Agent (å‘½ä»¤è¡Œé—®é¢˜æ¨¡å¼)----------------")
        print(f"é—®é¢˜: {args.question}")   


    print("--------------å¯åŠ¨ MCP Agent----------------")
    asyncio.run(main())

