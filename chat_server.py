import asyncio
import json
import os
from typing import AsyncGenerator
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from contextlib import asynccontextmanager
from pydantic import BaseModel
from openai import AsyncOpenAI
from dotenv import load_dotenv
from mcp.client.session import ClientSession
from mcp.client.streamable_http import streamablehttp_client
import logging

# å¯¼å…¥ç°æœ‰çš„MCP Agent
from myMcp import MCPAgent

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

# å…¨å±€å˜é‡
openai_client = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global openai_client
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    try:
        openai_client = AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL")
        )
        logger.info("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not os.getenv("OPENAI_API_KEY"):
            logger.warning("âš ï¸  OPENAI_API_KEY æœªè®¾ç½®")
        if not os.getenv("OPENAI_BASE_URL"):
            logger.warning("âš ï¸  OPENAI_BASE_URL æœªè®¾ç½®")
            
        mcp_url = os.getenv("MCP_SERVER_URL")
        if mcp_url:
            logger.info(f"ğŸ”— MCPæœåŠ¡å™¨URL: {mcp_url}")
        else:
            logger.info("â„¹ï¸  æœªé…ç½®MCP_SERVER_URLï¼Œå°†ä½¿ç”¨æ— MCPæ¨¡å¼")
            
    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        raise e
    
    yield  # åº”ç”¨è¿è¡Œ
    
    # å…³é—­æ—¶æ¸…ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
    logger.info("ğŸ”„ åº”ç”¨å…³é—­ä¸­...")

app = FastAPI(
    title="MCPèŠå¤©åŠ©æ‰‹", 
    description="æ”¯æŒå·¥å…·è°ƒç”¨çš„æµå¼èŠå¤©ç•Œé¢",
    lifespan=lifespan
)

# æŒ‚è½½é™æ€æ–‡ä»¶
app.mount("/static", StaticFiles(directory="static"), name="static")

# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    message: str

async def init_mcp_agent():
    """åˆå§‹åŒ–MCP Agent"""
    global openai_client, mcp_agent
    
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    openai_client = AsyncOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL")
    )
    
    mcp_server_url = os.getenv("MCP_SERVER_URL")
    
    if mcp_server_url:
        try:
            logger.info(f"å°è¯•è¿æ¥åˆ°MCPæœåŠ¡å™¨: {mcp_server_url}")
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿æŒè¿æ¥ï¼Œæ‰€ä»¥ä½¿ç”¨å…¨å±€è¿æ¥
            async with streamablehttp_client(mcp_server_url) as (read_stream, write_stream, _):
                async with ClientSession(read_stream, write_stream) as mcp_session:
                    await mcp_session.initialize()
                    
                    # è·å–å·¥å…·
                    tools_response = await mcp_session.list_tools()
                    available_tools = tools_response.tools
                    
                    logger.info(f"è¿æ¥æˆåŠŸï¼Œå¯ç”¨å·¥å…·æ•°é‡: {len(available_tools)}")
                    
                    # åˆ›å»ºMCP Agent
                    mcp_agent = MCPAgent(openai_client, mcp_session)
                    mcp_agent.available_tools = available_tools
                    
                    return mcp_agent
        except Exception as e:
            logger.error(f"MCPè¿æ¥å¤±è´¥: {e}")
            mcp_agent = MCPAgent(openai_client, None)
    else:
        logger.info("æœªè®¾ç½®MCP_SERVER_URLï¼Œä½¿ç”¨æ— MCPæ¨¡å¼")
        mcp_agent = MCPAgent(openai_client, None)
    
    return mcp_agent

class StreamingChatAgent:
    """æµå¼èŠå¤©ä»£ç†ï¼ŒåŸºäºç°æœ‰çš„MCPAgentè¿›è¡Œæµå¼æ”¹é€ """
    
    def __init__(self, openai_client, mcp_session=None):
        self.openai_client = openai_client
        self.mcp_session = mcp_session
        self.available_tools = []
    
    @property
    def mcp_available(self):
        return self.mcp_session is not None
    
    def get_openai_tools_schema(self):
        """å¤ç”¨MCPAgentçš„å·¥å…·schemaç”Ÿæˆé€»è¾‘"""
        if not self.mcp_available or not self.available_tools:
            return []
            
        try:
            with open("/root/python_learn/mcpServer/schemas.json", "r", encoding="utf-8") as f:
                schema_params = json.load(f)
        except FileNotFoundError:
            schema_params = {}
        
        openai_tools = []
        
        for tool in self.available_tools:
            tool_params = schema_params.get(tool.name, {})
            
            tool_schema = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description or f"MCP tool: {tool.name}",
                    "parameters": tool_params if tool_params else {
                        "type": "object",
                        "properties": {},
                        "required": []
                    }
                }
            }        
            openai_tools.append(tool_schema)
        
        return openai_tools
    
    async def call_mcp_tool(self, tool_name, parameters):
        """è°ƒç”¨MCPå·¥å…·"""
        if not self.mcp_available or not self.mcp_session:
            raise Exception("MCPæœåŠ¡å™¨ä¸å¯ç”¨")
            
        try:    
            result = await self.mcp_session.call_tool(tool_name, parameters)
            if result.content and len(result.content) > 0:
                return result.content[0].text
            else:
                return "å·¥å…·è°ƒç”¨æˆåŠŸï¼Œä½†æ— è¿”å›ç»“æœ"
        except Exception as e:
            logger.error(f"MCPå·¥å…·è°ƒç”¨å¼‚å¸¸: {e}")
            raise e
    
    async def stream_chat_with_tools(self, user_message: str) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨"""
        
        # å‘é€å¼€å§‹ä¿¡å·
        yield json.dumps({
            "type": "start",
            "message": "å¼€å§‹å¤„ç†æ‚¨çš„é—®é¢˜..."
        }, ensure_ascii=False) + "\n"
        
        # æ„å»ºæ¶ˆæ¯
        system_content = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ã€‚"
        if self.mcp_available and self.available_tools:
            system_content += "ä½ æœ‰ä¸€äº›å·¥å…·å¯ä»¥å¸®åŠ©è·å–å®æ—¶ä¿¡æ¯ï¼Œå¦‚å¤©æ°”æŸ¥è¯¢ç­‰ã€‚"
        
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_message}
        ]
        
        tools = self.get_openai_tools_schema()
        
        try:
            # é¦–æ¬¡è°ƒç”¨OpenAI
            if tools:
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    tools=tools,
                    tool_choice="auto",
                    max_tokens=500
                )
            else:
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    max_tokens=500
                )
            
            response_message = response.choices[0].message
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            if response_message.tool_calls:
                # å‘é€å·¥å…·è°ƒç”¨ä¿¡æ¯
                yield json.dumps({
                    "type": "tool_calls",
                    "tools": [
                        {
                            "name": tool_call.function.name,
                            "arguments": json.loads(tool_call.function.arguments)
                        }
                        for tool_call in response_message.tool_calls
                    ]
                }, ensure_ascii=False) + "\n"
                
                # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
                messages.append({
                    "role": "assistant",
                    "content": response_message.content,
                    "tool_calls": [{
                        "id": tool_call.id,
                        "type": tool_call.type,
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    } for tool_call in response_message.tool_calls]
                })
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                for tool_call in response_message.tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)
                    
                    yield json.dumps({
                        "type": "tool_executing",
                        "tool_name": function_name,
                        "message": f"æ­£åœ¨è°ƒç”¨å·¥å…·: {function_name}"
                    }, ensure_ascii=False) + "\n"
                    
                    try:
                        tool_result = await self.call_mcp_tool(function_name, function_args)
                        
                        yield json.dumps({
                            "type": "tool_result",
                            "tool_name": function_name,
                            "result": str(tool_result)
                        }, ensure_ascii=False) + "\n"
                        
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": str(tool_result)
                        })
                    except Exception as e:
                        error_msg = f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"
                        yield json.dumps({
                            "type": "tool_error",
                            "tool_name": function_name,
                            "error": error_msg
                        }, ensure_ascii=False) + "\n"
                        
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": error_msg
                        })
                
                # ç¬¬äºŒæ¬¡è°ƒç”¨OpenAIï¼Œè·å–æµå¼å“åº”
                yield json.dumps({
                    "type": "generating",
                    "message": "æ­£åœ¨ç”Ÿæˆå›ç­”..."
                }, ensure_ascii=False) + "\n"
                
                final_response = await self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    max_tokens=300,
                    stream=True
                )
                
                # æµå¼è¾“å‡ºæœ€ç»ˆå›ç­”
                async for chunk in final_response:
                    if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                        yield json.dumps({
                            "type": "content",
                            "content": chunk.choices[0].delta.content
                        }, ensure_ascii=False) + "\n"
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥æµå¼è¾“å‡º
                yield json.dumps({
                    "type": "generating",
                    "message": "æ­£åœ¨ç”Ÿæˆå›ç­”..."
                }, ensure_ascii=False) + "\n"
                
                # é‡æ–°è°ƒç”¨è·å–æµå¼å“åº”
                if tools:
                    stream_response = await self.openai_client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=messages,
                        tools=tools,
                        tool_choice="auto",
                        max_tokens=500,
                        stream=True
                    )
                else:
                    stream_response = await self.openai_client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=messages,
                        max_tokens=500,
                        stream=True
                    )
                
                async for chunk in stream_response:
                    if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta.content:
                        yield json.dumps({
                            "type": "content",
                            "content": chunk.choices[0].delta.content
                        }, ensure_ascii=False) + "\n"
            
            # å‘é€ç»“æŸä¿¡å·
            yield json.dumps({
                "type": "end",
                "message": "å›ç­”å®Œæˆ"
            }, ensure_ascii=False) + "\n"
            
        except Exception as e:
            logger.error(f"æµå¼èŠå¤©å¤„ç†é”™è¯¯: {type(e).__name__}: {str(e)}")
            import traceback
            traceback.print_exc()
            yield json.dumps({
                "type": "error",
                "error": f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}"
            }, ensure_ascii=False) + "\n"

# å…¨å±€æµå¼èŠå¤©ä»£ç†
streaming_agent = None

async def get_streaming_agent():
    """è·å–æµå¼èŠå¤©ä»£ç†"""
    global streaming_agent, openai_client
    
    if streaming_agent is None:
        if openai_client is None:
            openai_client = AsyncOpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL")
            )
        
        mcp_server_url = os.getenv("MCP_SERVER_URL")
        
        if mcp_server_url:
            try:
                # è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æ¯æ¬¡éƒ½åˆ›å»ºæ–°è¿æ¥
                # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåº”è¯¥ç»´æŠ¤é•¿è¿æ¥
                streaming_agent = StreamingChatAgent(openai_client, None)
                logger.info("åˆ›å»ºæ— MCPè¿æ¥çš„æµå¼ä»£ç†")
            except Exception as e:
                logger.error(f"åˆ›å»ºæµå¼ä»£ç†å¤±è´¥: {e}")
                streaming_agent = StreamingChatAgent(openai_client, None)
        else:
            streaming_agent = StreamingChatAgent(openai_client, None)
    
    return streaming_agent

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """æµå¼èŠå¤©æ¥å£"""
    try:
        # ç¡®ä¿OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
        global openai_client
        if openai_client is None:
            api_key = os.getenv("OPENAI_API_KEY")
            base_url = os.getenv("OPENAI_BASE_URL")
            
            if not api_key:
                raise HTTPException(
                    status_code=500, 
                    detail="OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶é…ç½®"
                )
            if not base_url:
                raise HTTPException(
                    status_code=500, 
                    detail="OPENAI_BASE_URL ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶é…ç½®"
                )
                
            openai_client = AsyncOpenAI(
                api_key=api_key,
                base_url=base_url
            )
            logger.info("OpenAIå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
        
        # ä¸ºäº†æ”¯æŒMCPè¿æ¥ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¯æ¬¡è¯·æ±‚æ—¶åˆ›å»ºæ–°çš„è¿æ¥
        mcp_server_url = os.getenv("MCP_SERVER_URL")
        
        if mcp_server_url:
            try:
                async def generate_with_mcp():
                    async with streamablehttp_client(mcp_server_url) as (read_stream, write_stream, _):
                        async with ClientSession(read_stream, write_stream) as mcp_session:
                            await mcp_session.initialize()
                            
                            # è·å–å·¥å…·
                            tools_response = await mcp_session.list_tools()
                            available_tools = tools_response.tools
                            
                            # åˆ›å»ºæµå¼ä»£ç†
                            agent = StreamingChatAgent(openai_client, mcp_session)
                            agent.available_tools = available_tools
                            
                            # æµå¼ç”Ÿæˆå“åº”
                            async for chunk in agent.stream_chat_with_tools(request.message):
                                yield chunk
                
                return StreamingResponse(
                    generate_with_mcp(),
                    media_type="text/plain",
                    headers={"Cache-Control": "no-cache"}
                )
            except Exception as e:
                logger.error(f"MCPè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æ— MCPæ¨¡å¼: {e}")
        
        # æ— MCPæ¨¡å¼
        agent = StreamingChatAgent(openai_client, None)
        
        return StreamingResponse(
            agent.stream_chat_with_tools(request.message),
            media_type="text/plain",
            headers={"Cache-Control": "no-cache"}
        )
        
    except Exception as e:
        logger.error(f"èŠå¤©æµå¤„ç†é”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def get_chat_page():
    """è¿”å›èŠå¤©é¡µé¢"""
    return FileResponse("templates/chat.html")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    global openai_client
    
    status = {
        "status": "ok",
        "message": "MCPèŠå¤©æœåŠ¡å™¨è¿è¡Œæ­£å¸¸",
        "openai_client": "å·²åˆå§‹åŒ–" if openai_client else "æœªåˆå§‹åŒ–",
        "mcp_server_url": os.getenv("MCP_SERVER_URL", "æœªé…ç½®"),
        "environment": {
            "openai_api_key": "å·²é…ç½®" if os.getenv("OPENAI_API_KEY") else "æœªé…ç½®",
            "openai_base_url": os.getenv("OPENAI_BASE_URL", "æœªé…ç½®")
        }
    }
    
    return status

@app.post("/test")
async def test_simple_chat():
    """ç®€å•çš„æµ‹è¯•æ¥å£ï¼Œä¸ä½¿ç”¨æµå¼å“åº”"""
    try:
        global openai_client
        if openai_client is None:
            openai_client = AsyncOpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL")
            )
        
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "è¯·ç®€å•å›å¤ï¼šä½ å¥½"}],
            max_tokens=50
        )
        
        return {
            "status": "success",
            "response": response.choices[0].message.content,
            "message": "OpenAIè¿æ¥æ­£å¸¸"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "message": "OpenAIè¿æ¥å¤±è´¥"
        }

if __name__ == "__main__":
    import uvicorn
    
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    if openai_client is None:
        openai_client = AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL")
        )
    
    logger.info("å¯åŠ¨MCPèŠå¤©æœåŠ¡å™¨...")
    logger.info("è®¿é—® http://localhost:8002 å¼€å§‹èŠå¤©")
    
    uvicorn.run(
        "chat_server:app",
        host="0.0.0.0",
        port=8002,
        reload=True,
        log_level="info"
    )
